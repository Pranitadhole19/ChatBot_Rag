{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11666061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain faiss-cpu transformers sentence-transformers pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f043d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from transformers import AutoTokenizer\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02f614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local cache folder for HuggingFace model\n",
    "CACHE_DIR = os.path.normpath(os.path.join(os.getcwd(), \"models\"))\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L12-v2\", device=\"cpu\"):\n",
    "        self.embedding_function = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            cache_folder=CACHE_DIR,\n",
    "            model_kwargs={\"device\": device},\n",
    "        )\n",
    "\n",
    "class FaissDb:\n",
    "    def __init__(self, docs, embedding_function):\n",
    "        self.db = FAISS.from_documents(\n",
    "            docs, embedding_function, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "\n",
    "    def similarity_search(self, question: str, k: int = 3):\n",
    "        return self.db.similarity_search(question, k=k)\n",
    "\n",
    "def load_and_split_pdfs(file_paths: list, chunk_size: int = 512):\n",
    "    loaders = [PyPDFLoader(file_path) for file_path in file_paths]\n",
    "    pages = []\n",
    "    for loader in loaders:\n",
    "        pages.extend(loader.load())\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\"),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=64,\n",
    "        strip_whitespace=True,\n",
    "    )\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8637113c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a288af81594a508c6aa6b1ce45bd1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='C:\\\\Users\\\\Pranita\\\\OneDrive\\\\Desktop\\\\Pranita\\\\Pranita-cv\\\\Pranita_Dhole_CV_Recr‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upload = widgets.FileUpload(accept=r\"C:\\Users\\Pranita\\OneDrive\\Desktop\\Data Analysis\\SQL\\Theory.pdf\", multiple=False)\n",
    "display(upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc367b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 68 document chunks.\n",
      "PDF loaded and indexed with 68 chunks.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "# Save the uploaded PDF\n",
    "file_path = r\"C:\\Users\\Pranita\\OneDrive\\Desktop\\Data Analysis\\SQL\\Theory.pdf\"\n",
    "for filename, fileinfo in upload.value.items():\n",
    "    file_path = filename\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(fileinfo['content'])\n",
    "\n",
    "# Load and split\n",
    "docs = load_and_split_pdfs([file_path])\n",
    "\n",
    "# Check if any text was extracted\n",
    "if not docs:\n",
    "    raise ValueError(\"No text could be extracted from the uploaded PDF. Please try with a different file.\")\n",
    "\n",
    "print(f\"Loaded {len(docs)} document chunks.\")\n",
    "\n",
    "encoder = Encoder()\n",
    "\n",
    "# Try generating embeddings\n",
    "texts = [doc.page_content for doc in docs]\n",
    "embeddings = encoder.embedding_function.embed_documents(texts)\n",
    "\n",
    "# Check if embeddings were created\n",
    "if not embeddings:\n",
    "    raise ValueError(\"Embeddings generation failed. Please check your embedding function or input texts.\")\n",
    "\n",
    "# Proceed only if everything is valid\n",
    "faiss_db = FaissDb(docs, encoder.embedding_function)\n",
    "\n",
    "print(f\"PDF loaded and indexed with {len(docs)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6b4843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (type 'exit' to stop): what is this file about?\n",
      "\n",
      "üîç Top Relevant Answers:\n",
      "\n",
      "1. Created by: Vinay Kumar Panika\n",
      "Key Points:\n",
      "Used to process row-by-row results.\n",
      "Slower than set-based operations.\n",
      "Helps in complex data manipulation.\n",
      "Not recommended for large datasets.Example:\n",
      "\n",
      "2. DELETE TRUNCATE\n",
      "Removes specific rows based on a\n",
      "condition using the WHERE clause.Removes all rows from the table without\n",
      "any condition.\n",
      "Can be rolled back using ROLLBACK if\n",
      "inside a transaction.Cannot be rolled back once executed.\n",
      "Slower because it logs each row deletion.Faster because it does not log individual\n",
      "row deletions.\n",
      "Maintains table structure and identity\n",
      "column values.Resets identity column values to the initial\n",
      "seed.\n",
      "Index Fragmentation occurs when the logical order of index pages i\n",
      "\n",
      "3. Created by: Vinay Kumar Panika2. RANK()\n",
      "Assigns a rank to each row with the same values having the same rank, but skips ranks for\n",
      "duplicate values.\n",
      "Syntax:\n",
      "3. DENSE_RANK()\n",
      "Similar to RANK(), but does not skip ranks for duplicate values.\n",
      "Syntax:\n",
      "4. NTILE(n)\n",
      "Divides the result set into n equal parts and assigns a group number to each row.\n",
      "Syntax:\n",
      "5. SUM()\n",
      "Calculates the cumulative total of a column within a partition.\n",
      "Syntax:\n",
      "6. AVG()\n",
      "Calculates the average value of a column within a partition.\n",
      "Sy\n",
      "\n",
      "\n",
      "Ask a question (type 'exit' to stop): exit\n",
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"\\nAsk a question (type 'exit' to stop): \")\n",
    "    if query.lower() == 'exit':\n",
    "        print(\"Chat ended.\")\n",
    "        break\n",
    "\n",
    "    results = faiss_db.similarity_search(query)\n",
    "    print(\"\\nüîç Top Relevant Answers:\\n\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"{i}. {doc.page_content.strip()[:500]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850cfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
